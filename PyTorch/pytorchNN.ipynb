{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DFSA1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a basic neural network using pytorch\n",
        "\n",
        "PyTorch provides **nn module** that makes building neural networks much easier."
      ],
      "metadata": {
        "id": "l3UZS2jk69l_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oPkLJIUl6Kwj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for our network\n",
        "input_size = 32  # number of input units\n",
        "hidden_sizes = [16, 8] # two layers of hidden units\n",
        "output_size = 1 # number of output units\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "uIh4EyiU50BL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a sample dataset\n",
        "x = torch.randn(batch_size, input_size)\n",
        "y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0]])"
      ],
      "metadata": {
        "id": "jnMVsSiB54aQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a feed-forward network\n",
        "'''\n",
        "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations:\n",
        "Here our model contains: 784 input units, a hidden layer with 128 units, ReLU activation, 64 unit hidden layer, another ReLU, then the output layer with 10 units, and the softmax output.\n",
        "ReLU and softmax are activation functions.\n",
        "'''\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.Sigmoid())\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qPUMk86Xk6",
        "outputId": "2e4a28af-f519-4264-c1be-48a514b5f9c2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stochastic gradient descent optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)"
      ],
      "metadata": {
        "id": "jI6BwbgJ6q9X"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function\n",
        "lf = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "fBrsO_gw7cPI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent Algorithm\n",
        "for epoch in range(20):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_prediction = model(x)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = lf(y_prediction, y)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpT4O8yG7rcR",
        "outputId": "9444ad09-1977-4f0c-c6f3-129a0f95c0bc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.24759051203727722\n",
            "epoch:  1  loss:  0.24749794602394104\n",
            "epoch:  2  loss:  0.247406005859375\n",
            "epoch:  3  loss:  0.24731458723545074\n",
            "epoch:  4  loss:  0.2472236156463623\n",
            "epoch:  5  loss:  0.24713310599327087\n",
            "epoch:  6  loss:  0.24704304337501526\n",
            "epoch:  7  loss:  0.24695342779159546\n",
            "epoch:  8  loss:  0.24686460196971893\n",
            "epoch:  9  loss:  0.24677713215351105\n",
            "epoch:  10  loss:  0.24669049680233002\n",
            "epoch:  11  loss:  0.24660415947437286\n",
            "epoch:  12  loss:  0.24651817977428436\n",
            "epoch:  13  loss:  0.24643263220787048\n",
            "epoch:  14  loss:  0.24634747207164764\n",
            "epoch:  15  loss:  0.24626271426677704\n",
            "epoch:  16  loss:  0.2461784929037094\n",
            "epoch:  17  loss:  0.24609465897083282\n",
            "epoch:  18  loss:  0.24601277709007263\n",
            "epoch:  19  loss:  0.24593214690685272\n"
          ]
        }
      ]
    }
  ]
}